{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import process_time \n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=process_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Read dataframe, normalise and merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location=r\"D:\\Siddharth Data\\PLAsTiCC\\Raw Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(fr\"{data_location}\\training_set.csv\")\n",
    "df_metadata=pd.read_csv(fr\"{data_location}\\training_set_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Comment and uncomment as required\n",
    "\n",
    "# all_flux=df.loc[:,\"flux\"].values\n",
    "# all_flux_err=df.loc[:,\"flux_err\"].values\n",
    "\n",
    "# all_hostgal_specz=df_metadata.loc[:,\"hostgal_specz\"].values\n",
    "# all_hostgal_photoz=df_metadata.loc[:,\"hostgal_photoz\"].values\n",
    "# all_hostgal_photoz_err=df_metadata.loc[:,\"hostgal_photoz_err\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment and uncomment as required\n",
    "\n",
    "# flux_mean=np.mean(all_flux)\n",
    "# flux_err_mean=np.mean(all_flux_err)\n",
    "# flux_std=np.std(all_flux)\n",
    "# flux_err_std=np.std(all_flux_err)\n",
    "\n",
    "# hostgal_specz_mean=np.mean(all_hostgal_specz)\n",
    "# hostgal_photoz_mean=np.mean(all_hostgal_photoz)\n",
    "# hostgal_photoz_err_mean=np.mean(all_hostgal_photoz_err)\n",
    "# hostgal_specz_std=np.std(all_hostgal_specz)\n",
    "# hostgal_photoz_std=np.std(all_hostgal_photoz)\n",
    "# hostgal_photoz_err_std=np.std(all_hostgal_photoz_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment and uncomment as required\n",
    "\n",
    "# df.loc[:,\"flux\"]=(all_flux - flux_mean)/flux_std\n",
    "# df.loc[:,\"flux_err\"]=(all_flux_err - flux_err_mean)/flux_err_std\n",
    "\n",
    "# df_metadata.loc[:,\"hostgal_specz\"]=(all_hostgal_specz - hostgal_specz_mean)/hostgal_specz_std\n",
    "# df_metadata.loc[:,\"hostgal_photoz\"]=(all_hostgal_photoz - hostgal_photoz_mean)/hostgal_photoz_std\n",
    "# df_metadata.loc[:,\"hostgal_photoz_err\"]=(all_hostgal_photoz_err - hostgal_photoz_err_mean)/hostgal_photoz_err_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.merge(df_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df_metadata)\n",
    "\n",
    "#Comment and uncomment as required\n",
    "\n",
    "# del(all_flux)\n",
    "# del(all_flux_err)\n",
    "# del(all_hostgal_specz)\n",
    "# del(all_hostgal_photoz)\n",
    "# del(all_hostgal_photoz_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Compute (cumulative) time difference between each observation for each object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate time diff between all observations in mjd\n",
    "df[\"mjd_diff\"]=df['mjd'].diff()\n",
    "df[\"mjd_diff\"]=df[\"mjd_diff\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find indexes where new objects appear, and set the mjd_diff for this to 0\n",
    "obj_change_index=np.where(df[\"object_id\"].values[:-1] != df[\"object_id\"].values[1:])[0] + 1\n",
    "df.loc[obj_change_index, ['mjd_diff']]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupby method to find seperate cumsums for all objects\n",
    "df[\"cumulative_mjd_diff\"]=df.loc[:,[\"object_id\", \"mjd_diff\"]].groupby(\"object_id\").cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Convert given targets to standard form (0-14) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use dictionary to create new column, replacing targets(6,15,...) by class(0,1,...)\n",
    "target_dict ={\n",
    "    6: 0,\n",
    "    15: 1,\n",
    "    16: 2,\n",
    "    42: 3,\n",
    "    52: 4,\n",
    "    53: 5,\n",
    "    62: 6,\n",
    "    64: 7,\n",
    "    65: 8,\n",
    "    67: 9,\n",
    "    88: 10,\n",
    "    90: 11,\n",
    "    92: 12,\n",
    "    95: 13,\n",
    "    99: 14\n",
    "}\n",
    "df[\"target_class\"]=df.loc[:,[\"target\"]].replace(target_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Group MJDs within 1 night of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mjd_arr=df[\"mjd\"].values\n",
    "time_diff_arr=df[\"mjd_diff\"].values\n",
    "grouped_mjd_arr=np.zeros_like(mjd_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_time=0\n",
    "for i in range(len(mjd_arr)):\n",
    "    current_time=mjd_arr[i]\n",
    "    time_diff=time_diff_arr[i]\n",
    "    if time_diff==0 or current_time-prev_time>0.33:\n",
    "        grouped_mjd_arr[i]=current_time\n",
    "        prev_time=current_time\n",
    "    else:\n",
    "        grouped_mjd_arr[i]=prev_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"grouped_mjd\"]=grouped_mjd_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(grouped_mjd_arr)\n",
    "del(time_diff_arr)\n",
    "del(mjd_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: For observations of the same passband within 1 night of each other, choose the one with least flux_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(\"flux_err\").groupby([\"object_id\", \"grouped_mjd\", \"passband\"]).first()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create passband column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all unnecessary columns. Note : mjd_diff and cumulative_mjd_diff are dropped as cause problems when pivoting. Will recalculate later\n",
    "df = df.drop(\n",
    "    [\n",
    "        \"mjd\",\n",
    "        \"detected\",\n",
    "        \"ra\",\n",
    "        \"decl\",\n",
    "        \"gal_b\",\n",
    "        \"gal_l\",\n",
    "        \"mjd_diff\",\n",
    "        \"cumulative_mjd_diff\",\n",
    "#         \"ddf\",                      #Experiment with these last 3\n",
    "#         \"distmod\",\n",
    "#         \"mwebv\"\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_features=[\"ddf\",\"hostgal_specz\",\"hostgal_photoz\",\"hostgal_photoz_err\",\"distmod\",\"mwebv\"]\n",
    "mini_df=df[[\"object_id\"] + fixed_features ].groupby(\"object_id\").first()\n",
    "\n",
    "df=df.drop(mini_df,axis=1)\n",
    "df = pd.pivot_table(df, index=[\"object_id\",\"grouped_mjd\",\"target\",\"target_class\"], columns=[\"passband\"])\n",
    "df.columns= [f\"{tup[0]}_passband_{tup[1]}\" for tup in df.columns.values]\n",
    "df=df.reset_index([\"grouped_mjd\",\"target\",\"target_class\"])\n",
    "df=df.join(mini_df,how=\"left\")\n",
    "\n",
    "del(mini_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={\"grouped_mjd\": \"mjd\"})\n",
    "df=df.reset_index()\n",
    "\n",
    "\n",
    "#Calculate time diff between all observations in mjd\n",
    "df[\"mjd_diff\"]=df['mjd'].diff()\n",
    "df[\"mjd_diff\"]=df[\"mjd_diff\"].fillna(0)\n",
    "#Find indexes where new objects appear, and set the mjd_diff for this to 0\n",
    "obj_change_index=np.where(df[\"object_id\"].values[:-1] != df[\"object_id\"].values[1:])[0] + 1\n",
    "df.loc[obj_change_index, ['mjd_diff']]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUTURE: Try making the time series uniform everywhere by inputting 0 everyday when data wasn't measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Convert dataframe into list of form [(nparray,target_class,obj_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.set_index([\"object_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(\n",
    "    [\n",
    "        \"mjd\",\n",
    "        \"target\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "df = df.fillna(0)\n",
    "#FUTURE: Try filling -1 and see if performance improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save some memory by converting float64 to float32 as 32 enough\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == np.float64:\n",
    "        df[col] = df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recalculate time diff between all rows and set new objs to zero as before\n",
    "all_obj_ids=np.unique(df.index.get_level_values(0).values)\n",
    "dfarray=df.reset_index().to_numpy()\n",
    "all_obj_ids_long=dfarray[0:,0]\n",
    "all_labels_long=dfarray[0:,1]\n",
    "obj_change_index=np.where(all_obj_ids_long[:-1] != all_obj_ids_long[1:])[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuplist=list(zip(np.insert(obj_change_index,0,0),obj_change_index))\n",
    "list_of_data_arrays=[]\n",
    "for tup in tuplist:\n",
    "    list_of_data_arrays.append((dfarray[tup[0]:tup[1],2:],int(dfarray[tup[0],1]),int(dfarray[tup[0],0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_of_data_arrays              #REQUIRED LIST OF ARRAYS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(all_labels_long)\n",
    "del(all_obj_ids)\n",
    "del(all_obj_ids_long)\n",
    "del(df)\n",
    "del(dfarray)\n",
    "del(obj_change_index)\n",
    "del(tuplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing took 23.453125 seconds.\n"
     ]
    }
   ],
   "source": [
    "t2=process_time()\n",
    "print(f\"Preprocessing took {t2-t1} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\Sid\\Desktop\\bigpickle\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(list_of_data_arrays, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout,Bidirectional,Conv1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.backend import clear_session\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    list_of_data_arrays\n",
    "except NameError:\n",
    "    with open(r\"C:\\Users\\Sid\\Desktop\\bigpickle\", \"rb\") as fp:   # Unpickling\n",
    "        list_of_data_arrays=pickle.load(fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle=random.sample(list_of_data_arrays, len(list_of_data_arrays))\n",
    "del(list_of_data_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_arr=np.array(shuffle)[0:,0]\n",
    "shuffle_labels=np.array(shuffle)[0:,1]\n",
    "shuffle_obj_ids=np.array(shuffle)[0:,2]\n",
    "del(shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 162, 64)           21504     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                975       \n",
      "=================================================================\n",
      "Total params: 55,503\n",
      "Trainable params: 55,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_length=max(map(len,shuffle_arr))\n",
    "\n",
    "x_train = sequence.pad_sequences(shuffle_arr[:7066], maxlen=max_length)\n",
    "x_val = sequence.pad_sequences(shuffle_arr[7066:], maxlen=max_length)\n",
    "y_train=to_categorical(shuffle_labels[:7066], num_classes=15)\n",
    "y_val=to_categorical(shuffle_labels[7066:], num_classes=15)\n",
    "\n",
    "clear_session()\n",
    "\n",
    "data_dim = 19\n",
    "timesteps = max_length\n",
    "num_classes = 15\n",
    "\n",
    "# expected input data shape: (batch_size, timesteps, data_dim)\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True,\n",
    "               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\n",
    "# model.add(LSTM(64, return_sequences=True))  # returns a sequence of vectors of dimension 32\n",
    "model.add(LSTM(64))  # return a single vector of dimension 32\n",
    "model.add(Dense(15, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7066 samples, validate on 781 samples\n",
      "Epoch 1/10\n",
      "7066/7066 [==============================] - 177s 25ms/step - loss: 1.3544 - accuracy: 0.5436 - val_loss: 1.2336 - val_accuracy: 0.5787\n",
      "Epoch 2/10\n",
      "7066/7066 [==============================] - 178s 25ms/step - loss: 1.1025 - accuracy: 0.6163 - val_loss: 1.0873 - val_accuracy: 0.6504\n",
      "Epoch 3/10\n",
      "7066/7066 [==============================] - 177s 25ms/step - loss: 0.9918 - accuracy: 0.6584 - val_loss: 0.9787 - val_accuracy: 0.6517\n",
      "Epoch 4/10\n",
      "7066/7066 [==============================] - 177s 25ms/step - loss: 0.8992 - accuracy: 0.6847 - val_loss: 0.9376 - val_accuracy: 0.6825\n",
      "Epoch 5/10\n",
      "7066/7066 [==============================] - 177s 25ms/step - loss: 0.8349 - accuracy: 0.7022 - val_loss: 0.8831 - val_accuracy: 0.6940\n",
      "Epoch 6/10\n",
      "7066/7066 [==============================] - 177s 25ms/step - loss: 0.7932 - accuracy: 0.7170 - val_loss: 0.8386 - val_accuracy: 0.7042\n",
      "Epoch 7/10\n",
      "7066/7066 [==============================] - 178s 25ms/step - loss: 0.7737 - accuracy: 0.7202 - val_loss: 0.8162 - val_accuracy: 0.7196\n",
      "Epoch 8/10\n",
      "7066/7066 [==============================] - 178s 25ms/step - loss: 0.7535 - accuracy: 0.7308 - val_loss: 0.8362 - val_accuracy: 0.7119\n",
      "Epoch 9/10\n",
      "7066/7066 [==============================] - 177s 25ms/step - loss: 0.7223 - accuracy: 0.7440 - val_loss: 0.7802 - val_accuracy: 0.7311\n",
      "Epoch 10/10\n",
      "7066/7066 [==============================] - 194s 27ms/step - loss: 0.6864 - accuracy: 0.7573 - val_loss: 0.7932 - val_accuracy: 0.7298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1cc0f127f88>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=4, epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 lstms, 64, 64:\n",
    "nadam - 62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 lstms, 64,64,64:\n",
    "adam - 57\n",
    "nadam - 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (1==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IGNORE CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size_5cross=int(len(shuffle_arr)/5)\n",
    "set1_idx=(0,step_size_5cross)\n",
    "set2_idx=(step_size_5cross,step_size_5cross*2)\n",
    "set3_idx=(step_size_5cross*2,step_size_5cross*3)\n",
    "set4_idx=(step_size_5cross*3,step_size_5cross*4)\n",
    "set5_idx=(step_size_5cross*4,len(shuffle_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871.8888888888889"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shuffle_arr)/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=max(map(len,shuffle_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00, -5.08469922e+03, -4.49535986e+03, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  1.20957117e+03,  2.02470178e+03, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  1.39561005e+01],\n",
       "       [ 0.00000000e+00, -3.90445264e+03, -3.19892896e+03, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  2.98259997e+00],\n",
       "       ...,\n",
       "       [ 5.44017792e+01,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  1.00380003e+00],\n",
       "       [-7.91674866e+02,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  1.00520003e+00],\n",
       "       [ 3.23605347e+02,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  1.00530005e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((shuffle_arr[set1_idx[0]:set1_idx[1]], shuffle_arr[set2_idx[0]:set2_idx[1]]))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00, -5.08469922e+03, -4.49535986e+03, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  1.20957117e+03,  2.02470178e+03, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  1.39561005e+01],\n",
       "       [ 0.00000000e+00, -3.90445264e+03, -3.19892896e+03, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  2.98259997e+00],\n",
       "       ...,\n",
       "       [ 5.44017792e+01,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  1.00380003e+00],\n",
       "       [-7.91674866e+02,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  1.00520003e+00],\n",
       "       [ 3.23605347e+02,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  8.99999961e-03,  1.00530005e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_arr[set2_idx[0]:set2_idx[1]][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last is Val, first 4 are Train\n",
    "\n",
    "for i in \n",
    "\n",
    "x_train = sequence.pad_sequences(train_arr, maxlen=max_length)\n",
    "x_val = sequence.pad_sequences(val_arr, maxlen=max_length)\n",
    "y_train=to_categorical(train_labels, num_classes=15)\n",
    "y_val=to_categorical(val_labels, num_classes=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
